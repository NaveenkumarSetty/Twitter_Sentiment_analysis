# -*- coding: utf-8 -*-
"""Twitter_nlp_16m.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z50dW9e27l6ZgrnCvvsGPoczHxjqhYmv
"""

!pip install nltk

import os
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
import multiprocessing
import nltk
from sklearn.model_selection import train_test_split
nltk.download("stopwords")

col_names=["target", "ids", "date", "flag", "user", "text"]
df1=pd.read_csv("/content/Tweets.csv",encoding="ISO-8859-1",names=col_names)

df1

df1.info()

df1.describe(include='all')

sns.countplot(x=df1["target"])

df1.drop([ "ids", "date", "flag", "user"],axis=1)

df1["target"]=df1["target"].replace(4,1)

df1.tail(20)

data=df1["text"]
labels=np.array(df1["target"])

from nltk.corpus import stopwords
import string
stop_words = set(stopwords.words('english'))
punctuations=list(string.punctuation)
stop_words.update(punctuations)

import re
def split_words(text):
    words=text.split()
    return words

def remove_punctuations(words):
    re_punc=re.compile('[%s]' % re.escape(string.punctuation))
    word_striped=[re_punc.sub('', w) for w in words]
    return word_striped

def keep_alphabetic(words):
    words = [word for word in words if word.isalpha()]
    return words

def to_lower_case(words):
    # convert to lower case
    words = [word.lower() for word in words]
    return words

def to_sentence(words):
    return ' '.join(words)

def remove_stopwords(words):
    stop_words = set(stopwords.words('english'))
    words = [w for w in words if not w in stop_words]
    return words

def tweet(words):
    tweet_tokenizer=nltk.tokenize.TweetTokenizer(strip_handles=True,reduce_len=True)
    tweet=tweet_tokenizer.tokenize(words)
    return tweet

def denoise_text(text):
    words = split_words(text)
    words = to_lower_case(words)
    words = remove_punctuations(words)
    words = keep_alphabetic(words)
    words = remove_stopwords(words)
    return to_sentence(words)

data = data.apply(denoise_text)

X_train, X_test, y_train, y_test = test = train_test_split(data, labels,test_size=0.20,
                                                           random_state=1,
                                                           stratify = labels)

tokenizer = Tokenizer(num_words=10000, oov_token = '<OOV>')
tokenizer.fit_on_texts(X_train)
word_index = tokenizer.word_index
VOCAB_SIZE = len(word_index)+1
VOCAB_SIZE

maxlen = max([len(x) for x in X_train])
maxlen=60

train_sequences = tokenizer.texts_to_sequences(X_train)
train_padded_sequences = pad_sequences(train_sequences,maxlen=maxlen,padding='post',truncating='post')
test_sequences = tokenizer.texts_to_sequences(X_test)
test_padded_sequences = pad_sequences(test_sequences,maxlen=maxlen,padding='post',truncating='post')

embedding_dim = 32
model = tf.keras.Sequential([
        tf.keras.layers.Embedding(VOCAB_SIZE+1, embedding_dim, input_length=maxlen),
        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),
        tf.keras.layers.Flatten(),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dropout(0.30),
        tf.keras.layers.Dense(embedding_dim,activation='relu'),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dropout(0.30),
        tf.keras.layers.Dense(32,activation='relu'),
        tf.keras.layers.Dense(1,activation='sigmoid'),
    ])

model.compile(loss = 'binary_crossentropy',
                optimizer = 'adam',
                metrics = ['accuracy'])

model.summary()

history =    model.fit(train_padded_sequences,
                                               y_train,
                                               validation_data = (test_padded_sequences, y_test),
                                               epochs = 3)

print(X_test.iloc[6],'label: ;',y_test[6])

tokenizer.sequences_to_texts(test_padded_sequences)[6]

model.predict(test_padded_sequences)[6]